{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iw1zSSJb3S4q",
    "outputId": "ab5d369a-ea21-4da6-e8a2-c5be93429740"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/homebrew/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in /opt/homebrew/lib/python3.11/site-packages (0.20.1)\n",
      "Requirement already satisfied: timm in /opt/homebrew/lib/python3.11/site-packages (1.0.12)\n",
      "Requirement already satisfied: SimpleITK in /opt/homebrew/lib/python3.11/site-packages (2.4.0)\n",
      "Requirement already satisfied: nibabel in /opt/homebrew/lib/python3.11/site-packages (5.3.2)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (1.26.2)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/lib/python3.11/site-packages (1.11.4)\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/lib/python3.11/site-packages (3.8.2)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/homebrew/lib/python3.11/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/lib/python3.11/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/homebrew/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/homebrew/lib/python3.11/site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: pyyaml in /opt/homebrew/lib/python3.11/site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in /opt/homebrew/lib/python3.11/site-packages (from timm) (0.26.2)\n",
      "Requirement already satisfied: safetensors in /opt/homebrew/lib/python3.11/site-packages (from timm) (0.4.5)\n",
      "Requirement already satisfied: importlib-resources>=5.12 in /opt/homebrew/lib/python3.11/site-packages (from nibabel) (6.4.5)\n",
      "Requirement already satisfied: packaging>=20 in /opt/homebrew/lib/python3.11/site-packages (from nibabel) (24.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (4.45.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from huggingface_hub->timm) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/homebrew/lib/python3.11/site-packages (from huggingface_hub->timm) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision timm SimpleITK nibabel numpy scipy matplotlib scikit-learn monai\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models.swin_transformer import SwinTransformer\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://zenodo.org/api/records/10159290/files-archive -O data.zip\n",
    "!unzip data.zip\n",
    "!unzip images.zip -d extracted_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "If21IJnv33Vf"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OsAicttL5xVz"
   },
   "outputs": [],
   "source": [
    "image_files = []\n",
    "\n",
    "# Find all the files in the extracted directory\n",
    "for root, _, files in os.walk('extracted_images/images'):\n",
    "    for file in files:\n",
    "        if file.endswith('.mha'):\n",
    "            image_files.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_files = []\n",
    "\n",
    "# Find all mask files in the extracted directory\n",
    "for root, _, files in os.walk('extracted_images/masks'):\n",
    "    for file in files:\n",
    "        if file.endswith('.mha'):  # Assuming masks have \"_mask\" in their filename\n",
    "            mask_files.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_mask_pairs = list(zip(image_files, mask_files))\n",
    "for image_file, mask_file in image_mask_pairs:\n",
    "    if os.path.basename(image_file) == os.path.basename(mask_file):\n",
    "        continue\n",
    "    else:\n",
    "        print(\"mismatched files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, image_mask_pairs, target_size=(128, 128, 128), transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the MRIDataset.\n",
    "\n",
    "        Args:\n",
    "            image_mask_pairs (list of tuples): List of (image_path, mask_path) pairs.\n",
    "            target_size (tuple): Desired size for the resampled images and masks (depth, height, width).\n",
    "            transform (callable, optional): Transformations to apply to the data.\n",
    "        \"\"\"\n",
    "        self.image_mask_pairs = image_mask_pairs\n",
    "        self.target_size = target_size\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_mask_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_file, mask_file = self.image_mask_pairs[idx]\n",
    "\n",
    "        # Load image and mask\n",
    "        image = sitk.ReadImage(image_file)\n",
    "        mask = sitk.ReadImage(mask_file)\n",
    "\n",
    "        # Resample image and mask\n",
    "        image = self.resample_image(image, self.target_size, sitk.sitkLinear)\n",
    "        mask = self.resample_image(mask, self.target_size, sitk.sitkNearestNeighbor)\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        image = sitk.GetArrayFromImage(image)\n",
    "        mask = sitk.GetArrayFromImage(mask)\n",
    "\n",
    "        # Normalize the image\n",
    "        image = (image - np.min(image)) / (np.max(image) - np.min(image))\n",
    "        image = torch.tensor(image, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "        mask = torch.tensor(mask, dtype=torch.long)  # Long type for classification tasks\n",
    "\n",
    "        if self.transform:\n",
    "            image, mask = self.transform(image, mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    @staticmethod\n",
    "    def resample_image(image, target_size, interpolator):\n",
    "        \"\"\"\n",
    "        Resamples a 3D image to the target size.\n",
    "\n",
    "        Args:\n",
    "            image (SimpleITK.Image): The input image.\n",
    "            target_size (tuple): Desired size (depth, height, width).\n",
    "            interpolator (SimpleITK.Interpolator): Interpolation method (e.g., sitk.sitkLinear or sitk.sitkNearestNeighbor).\n",
    "\n",
    "        Returns:\n",
    "            SimpleITK.Image: Resampled image.\n",
    "        \"\"\"\n",
    "        original_size = image.GetSize()\n",
    "        original_spacing = image.GetSpacing()\n",
    "\n",
    "        # Calculate target spacing\n",
    "        target_spacing = [\n",
    "            (original_size[i] * original_spacing[i]) / target_size[i]\n",
    "            for i in range(3)\n",
    "        ]\n",
    "\n",
    "        # Resample the image\n",
    "        resample = sitk.ResampleImageFilter()\n",
    "        resample.SetOutputSpacing(target_spacing)\n",
    "        resample.SetSize(target_size)\n",
    "        resample.SetInterpolator(interpolator)\n",
    "        resample.SetOutputOrigin(image.GetOrigin())\n",
    "        resample.SetOutputDirection(image.GetDirection())\n",
    "        resample.SetDefaultPixelValue(0)\n",
    "\n",
    "        return resample.Execute(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MRIDataset(image_mask_pairs=image_mask_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0quJCaY4BCJ"
   },
   "source": [
    "# 3D Swin Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swin3DSegmentation(nn.Module):\n",
    "    def __init__(self, input_channels=1, num_classes=3, img_size=(128, 128, 128), embed_dim=96, patch_size=4):\n",
    "        \"\"\"\n",
    "        3D segmentation model using SwinUNETR backbone.\n",
    "        Args:\n",
    "            input_channels (int): Number of input channels (e.g., 1 for grayscale MRI images).\n",
    "            num_classes (int): Number of output classes for segmentation.\n",
    "            img_size (tuple): Size of the 3D input image (D, H, W).\n",
    "            embed_dim (int): Embedding dimension of the Swin Transformer.\n",
    "            patch_size (int): Patch size for splitting the input volume.\n",
    "        \"\"\"\n",
    "        super(Swin3DSegmentation, self).__init__()\n",
    "        \n",
    "        # SwinUNETR backbone\n",
    "        self.swin_unetr = SwinUNETR(\n",
    "            img_size=img_size,  # 3D image size\n",
    "            in_channels=input_channels,\n",
    "            out_channels=num_classes,\n",
    "            feature_size=embed_dim,\n",
    "            use_checkpoint=True  # Enables gradient checkpointing for memory efficiency\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for 3D segmentation.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, C, D, H, W).\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (B, num_classes, D, H, W).\n",
    "        \"\"\"\n",
    "        return self.swin_unetr(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCBxmDzn3i68"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NPbj4Yp23k-I"
   },
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def forward(self, preds, targets):\n",
    "        smooth = 1e-5\n",
    "        intersection = torch.sum(preds * targets)\n",
    "        union = torch.sum(preds) + torch.sum(targets)\n",
    "        return 1 - (2.0 * intersection + smooth) / (union + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split image-mask pairs into training and validation\n",
    "train_pairs, val_pairs = train_test_split(image_mask_pairs, test_size=0.2, random_state=55)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MRIDataset(image_mask_pairs=train_pairs)\n",
    "val_dataset = MRIDataset(image_mask_pairs=val_pairs)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.networks.nets.swin_unetr SwinUNETR.__init__:img_size: Argument `img_size` has been deprecated since version 1.3. It will be removed in version 1.5. The img_size argument is not required anymore and checks on the input size are run during forward().\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/179 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Check device compatibility\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Initialize the model and move it to the appropriate device\n",
    "model = Swin3DSegmentation(input_channels=1, num_classes=3).to(device)\n",
    "\n",
    "# Define the training function with progress tracking\n",
    "def train(model, train_loader, val_loader, epochs, device):\n",
    "    \"\"\"\n",
    "    Trains the model on the given data loaders.\n",
    "    Args:\n",
    "        model: The neural network model to be trained.\n",
    "        train_loader: DataLoader for the training data.\n",
    "        val_loader: DataLoader for the validation data.\n",
    "        epochs: Number of epochs to train.\n",
    "        device: Device for training (e.g., 'cpu', 'cuda', 'mps').\n",
    "    \"\"\"\n",
    "    # Define loss function and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        epoch_loss = 0.0\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Training loop with progress bar\n",
    "        for images, masks in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "            images, masks = images.to(device), masks.to(device)  # Move data to device\n",
    "\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs, masks)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():  # Disable gradient calculations for validation\n",
    "            for images, masks in val_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# Call the train function\n",
    "train(model, train_loader, val_loader, epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-9fKMG73qy9"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JItCWpsU3qaw"
   },
   "outputs": [],
   "source": [
    "def dice_coefficient(preds, targets):\n",
    "    smooth = 1e-5\n",
    "    intersection = torch.sum(preds * targets)\n",
    "    union = torch.sum(preds) + torch.sum(targets)\n",
    "    return (2.0 * intersection + smooth) / (union + smooth)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
