{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iw1zSSJb3S4q",
    "outputId": "ab5d369a-ea21-4da6-e8a2-c5be93429740"
   },
   "outputs": [],
   "source": [
    "#!pip install torch torchvision timm SimpleITK nibabel numpy scipy matplotlib scikit-learn monai ‘monai[einops]’\n",
    "#!pip install git+https://github.com/DIAGNijmegen/Tiger.git@stable\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models.swin_transformer import SwinTransformer\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#brew install wget\n",
    "#brew install unzip\n",
    "# !wget https://zenodo.org/api/records/10159290/files-archive -O data.zip\n",
    "# !unzip data.zip\n",
    "# !unzip data/images.zip -d data/extracted_images\n",
    "# !unzip data/masks.zip -d data/extracted_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "If21IJnv33Vf"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "OsAicttL5xVz"
   },
   "outputs": [],
   "source": [
    "image_files = []\n",
    "\n",
    "# Find all the files in the extracted directory\n",
    "for root, _, files in os.walk('../../extracted_images/images'):\n",
    "    for file in files:\n",
    "        if file.endswith('.mha'):\n",
    "            image_files.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_files = []\n",
    "\n",
    "# Find all mask files in the extracted directory\n",
    "for root, _, files in os.walk('../../extracted_images/masks'):\n",
    "    for file in files:\n",
    "        if file.endswith('.mha'):  # Assuming masks have \"_mask\" in their filename\n",
    "            mask_files.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "image_mask_pairs = list(zip(image_files, mask_files))\n",
    "for image_file, mask_file in image_mask_pairs:\n",
    "    if os.path.basename(image_file) == os.path.basename(mask_file):\n",
    "        continue\n",
    "    else:\n",
    "        print(\"mismatched files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_padded_patches(volume, patch_size=(64, 64, 64), stride=(32, 32, 32)):\n",
    "    \"\"\"\n",
    "    Extract overlapping patches from a 3D volume with padding for patches that exceed the volume boundary.\n",
    "    Args:\n",
    "        volume (numpy array): Input 3D volume (depth, height, width).\n",
    "        patch_size (tuple): Size of the patch (depth, height, width).\n",
    "        stride (tuple): Stride for sliding window extraction.\n",
    "    Returns:\n",
    "        list: List of padded 3D patches.\n",
    "    \"\"\"\n",
    "    depth, height, width = volume.shape\n",
    "    patch_depth, patch_height, patch_width = patch_size\n",
    "    stride_d, stride_h, stride_w = stride\n",
    "\n",
    "    patches = []\n",
    "\n",
    "    for z in range(0, depth, stride_d):\n",
    "        for y in range(0, height, stride_h):\n",
    "            for x in range(0, width, stride_w):\n",
    "                # Extract patch and pad if necessary\n",
    "                patch = volume[z:z + patch_depth, y:y + patch_height, x:x + patch_width]\n",
    "                padded_patch = pad_volume(patch, patch_size)\n",
    "                patches.append(padded_patch)\n",
    "\n",
    "    return patches\n",
    "\n",
    "\n",
    "class MRIDatasetWithPatches(Dataset):\n",
    "    def __init__(self, image_mask_pairs, target_size=(64, 64, 64), stride=(32, 32, 32), transform=None):\n",
    "        \"\"\"\n",
    "        Dataset class for extracting padded patches.\n",
    "        Args:\n",
    "            image_mask_pairs (list): List of (image_path, mask_path) pairs.\n",
    "            target_size (tuple): Patch size for extraction.\n",
    "            stride (tuple): Stride for overlapping patches.\n",
    "            transform (callable, optional): Transformations to apply to the data.\n",
    "        \"\"\"\n",
    "        self.image_mask_pairs = image_mask_pairs\n",
    "        self.target_size = target_size\n",
    "        self.stride = stride\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_mask_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_file, mask_file = self.image_mask_pairs[idx]\n",
    "\n",
    "        # Load image and mask\n",
    "        image = sitk.ReadImage(image_file)\n",
    "        mask = sitk.ReadImage(mask_file)\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        image = sitk.GetArrayFromImage(image)\n",
    "        mask = sitk.GetArrayFromImage(mask)\n",
    "\n",
    "        # Extract padded patches\n",
    "        image_patches = extract_padded_patches(image, self.target_size, self.stride)\n",
    "        mask_patches = extract_padded_patches(mask, self.target_size, self.stride)\n",
    "\n",
    "        # Select a specific patch based on idx\n",
    "        patch_idx = idx % len(image_patches)  # Cycle through available patches\n",
    "        img_patch = image_patches[patch_idx]\n",
    "        mask_patch = mask_patches[patch_idx]\n",
    "\n",
    "        # Apply transformations (optional)\n",
    "        if self.transform:\n",
    "            img_patch, mask_patch = self.transform(img_patch, mask_patch)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        img_tensor = torch.tensor(img_patch, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "        mask_tensor = torch.tensor(mask_patch, dtype=torch.long)\n",
    "\n",
    "        return img_tensor, mask_tensor\n",
    "\n",
    "\n",
    "def pad_volume(volume, target_size):\n",
    "    \"\"\"\n",
    "    Pads a 3D volume to the target size.\n",
    "    Args:\n",
    "        volume (numpy array): Input 3D volume.\n",
    "        target_size (tuple): Target size (depth, height, width).\n",
    "    Returns:\n",
    "        numpy array: Padded volume.\n",
    "    \"\"\"\n",
    "    depth, height, width = volume.shape\n",
    "    target_depth, target_height, target_width = target_size\n",
    "\n",
    "    pad_depth = max(0, target_depth - depth)\n",
    "    pad_height = max(0, target_height - height)\n",
    "    pad_width = max(0, target_width - width)\n",
    "\n",
    "    padding = [\n",
    "        (0, pad_depth),  # Pad at the end only\n",
    "        (0, pad_height),\n",
    "        (0, pad_width),\n",
    "    ]\n",
    "\n",
    "    return np.pad(volume, padding, mode=\"constant\", constant_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, image_mask_pairs, target_size=(128, 128, 128), transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the MRIDataset.\n",
    "\n",
    "        Args:\n",
    "            image_mask_pairs (list of tuples): List of (image_path, mask_path) pairs.\n",
    "            target_size (tuple): Desired size for the resampled images and masks (depth, height, width).\n",
    "            transform (callable, optional): Transformations to apply to the data.\n",
    "        \"\"\"\n",
    "        self.image_mask_pairs = image_mask_pairs\n",
    "        self.target_size = target_size\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_mask_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_file, mask_file = self.image_mask_pairs[idx]\n",
    "\n",
    "        # Load image and mask\n",
    "        image = sitk.ReadImage(image_file)\n",
    "        mask = sitk.ReadImage(mask_file)\n",
    "\n",
    "        # Resample image and mask\n",
    "        image = self.resample_image(image, self.target_size, sitk.sitkLinear)\n",
    "        mask = self.resample_image(mask, self.target_size, sitk.sitkNearestNeighbor)\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        image = sitk.GetArrayFromImage(image)\n",
    "        mask = sitk.GetArrayFromImage(mask)\n",
    "\n",
    "        # Normalize the image\n",
    "        image = (image - np.min(image)) / (np.max(image) - np.min(image))\n",
    "        image = torch.tensor(image, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "        mask = torch.tensor(mask, dtype=torch.long)  # Long type for classification tasks\n",
    "\n",
    "        if self.transform:\n",
    "            image, mask = self.transform(image, mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    @staticmethod\n",
    "    def resample_image(image, target_size, interpolator):\n",
    "        \"\"\"\n",
    "        Resamples a 3D image to the target size.\n",
    "\n",
    "        Args:\n",
    "            image (SimpleITK.Image): The input image.\n",
    "            target_size (tuple): Desired size (depth, height, width).\n",
    "            interpolator (SimpleITK.Interpolator): Interpolation method (e.g., sitk.sitkLinear or sitk.sitkNearestNeighbor).\n",
    "\n",
    "        Returns:\n",
    "            SimpleITK.Image: Resampled image.\n",
    "        \"\"\"\n",
    "        original_size = image.GetSize()\n",
    "        original_spacing = image.GetSpacing()\n",
    "\n",
    "        # Calculate target spacing\n",
    "        target_spacing = [\n",
    "            (original_size[i] * original_spacing[i]) / target_size[i]\n",
    "            for i in range(3)\n",
    "        ]\n",
    "\n",
    "        # Resample the image\n",
    "        resample = sitk.ResampleImageFilter()\n",
    "        resample.SetOutputSpacing(target_spacing)\n",
    "        resample.SetSize(target_size)\n",
    "        resample.SetInterpolator(interpolator)\n",
    "        resample.SetOutputOrigin(image.GetOrigin())\n",
    "        resample.SetOutputDirection(image.GetDirection())\n",
    "        resample.SetDefaultPixelValue(0)\n",
    "\n",
    "        return resample.Execute(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MRIDatasetWithPatches(image_mask_pairs=image_mask_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split image-mask pairs into training and validation\n",
    "train_pairs, val_pairs = train_test_split(image_mask_pairs, test_size=0.2, random_state=55)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MRIDatasetWithPatches(image_mask_pairs=train_pairs)\n",
    "val_dataset = MRIDatasetWithPatches(image_mask_pairs=val_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0quJCaY4BCJ"
   },
   "source": [
    "# 3D Swin Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swin3DSegmentation(nn.Module):\n",
    "    def __init__(self, input_channels=1, num_classes=210, img_size=(64, 64, 64), embed_dim=96, patch_size=4):\n",
    "        \"\"\"\n",
    "        3D segmentation model using SwinUNETR backbone.\n",
    "        Args:\n",
    "            input_channels (int): Number of input channels (e.g., 1 for grayscale MRI images).\n",
    "            num_classes (int): Number of output classes for segmentation.\n",
    "            img_size (tuple): Size of the 3D input image (D, H, W).\n",
    "            embed_dim (int): Embedding dimension of the Swin Transformer.\n",
    "            patch_size (int): Patch size for splitting the input volume.\n",
    "        \"\"\"\n",
    "        super(Swin3DSegmentation, self).__init__()\n",
    "        \n",
    "        # SwinUNETR backbone\n",
    "        self.swin_unetr = SwinUNETR(\n",
    "            img_size=img_size,  # 3D image size\n",
    "            in_channels=input_channels,\n",
    "            out_channels=num_classes,\n",
    "            feature_size=embed_dim,\n",
    "            use_checkpoint=True  # Enables gradient checkpointing for memory efficiency\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for 3D segmentation.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, C, D, H, W).\n",
    "                              - B: Batch size\n",
    "                              - C: Number of input channels\n",
    "                              - D: Depth of the input image\n",
    "                              - H: Height of the input image\n",
    "                              - W: Width of the input image\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (B, num_classes, D, H, W).\n",
    "                          - B: Batch size\n",
    "                          - num_classes: Number of segmentation classes\n",
    "                          - D: Depth of the output image\n",
    "                          - H: Height of the output image\n",
    "                          - W: Width of the output image\n",
    "        \"\"\"\n",
    "        return self.swin_unetr(x)\n",
    "\n",
    "def save_model(model, path):\n",
    "    \"\"\"\n",
    "    Saves the model's state dictionary to the specified path.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model to be saved.\n",
    "        path: The file path where the model will be saved.\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs, device, save_path=\"trained_model.pth\"):\n",
    "    \"\"\"\n",
    "    Trains the model on the given data loaders and saves it after training.\n",
    "    Args:\n",
    "        model: The neural network model to be trained.\n",
    "        train_loader: DataLoader for the training data.\n",
    "        val_loader: DataLoader for the validation data.\n",
    "        epochs: Number of epochs to train.\n",
    "        device: Device for training (e.g., 'cpu', 'cuda', 'mps').\n",
    "        save_path: Path to save the trained model.\n",
    "    \"\"\"\n",
    "    # Define loss function and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        epoch_loss = 0.0\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Training loop with progress bar\n",
    "        for images, masks in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "            images, masks = images.to(device), masks.to(device)  # Move data to device\n",
    "\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs, masks)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():  # Disable gradient calculations for validation\n",
    "            for images, masks in val_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "    # Save the model after training\n",
    "    save_model(model, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCBxmDzn3i68"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class '__main__.MRIDatasetWithPatches'>: it's not the same object as __main__.MRIDatasetWithPatches",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Initialize the model and move it to the appropriate device\u001b[39;00m\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m Swin3DSegmentation(input_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m210\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 12\u001b[0m images, masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     13\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m train(model, train_loader, val_loader, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice, save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatch_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:484\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:415\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1138\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1131\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1138\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_spawn_posix.py:47\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, fp)\n\u001b[0;32m---> 47\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class '__main__.MRIDatasetWithPatches'>: it's not the same object as __main__.MRIDatasetWithPatches"
     ]
    }
   ],
   "source": [
    "mp.set_start_method('spawn', force=True)  # Ensure proper start method for multiprocessing\n",
    "# Check device compatibility\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=8, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Initialize the model and move it to the appropriate device\n",
    "model = Swin3DSegmentation(input_channels=1, num_classes=210).to(device)\n",
    "images, masks = next(iter(train_loader))\n",
    "images = images.to(device)\n",
    "train(model, train_loader, val_loader, epochs=10, device=device, save_path=\"patch_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-9fKMG73qy9"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.networks.nets.swin_unetr SwinUNETR.__init__:img_size: Argument `img_size` has been deprecated since version 1.3. It will be removed in version 1.5. The img_size argument is not required anymore and checks on the input size are run during forward().\n",
      "  warn_deprecated(argname, msg, warning_category)\n",
      "/var/folders/kh/06qpp3c15bn4xjm4k3qs643c0000gn/T/ipykernel_87041/3232614033.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"patch_model.pth\", map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Swin3DSegmentation(\n",
       "  (swin_unetr): SwinUNETR(\n",
       "    (swinViT): SwinTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv3d(1, 96, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (layers1): ModuleList(\n",
       "        (0): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MLPBlock(\n",
       "                (linear1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (linear2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (fn): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            (reduction): Linear(in_features=768, out_features=192, bias=False)\n",
       "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers2): ModuleList(\n",
       "        (0): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MLPBlock(\n",
       "                (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (fn): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            (reduction): Linear(in_features=1536, out_features=384, bias=False)\n",
       "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers3): ModuleList(\n",
       "        (0): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MLPBlock(\n",
       "                (linear1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (linear2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (fn): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            (reduction): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers4): ModuleList(\n",
       "        (0): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MLPBlock(\n",
       "                (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (fn): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            (reduction): Linear(in_features=6144, out_features=1536, bias=False)\n",
       "            (norm): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (encoder1): UnetrBasicBlock(\n",
       "      (layer): UnetResBlock(\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(1, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (norm1): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm2): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (conv3): Convolution(\n",
       "          (conv): Conv3d(1, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (norm3): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (encoder2): UnetrBasicBlock(\n",
       "      (layer): UnetResBlock(\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (norm1): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm2): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (encoder3): UnetrBasicBlock(\n",
       "      (layer): UnetResBlock(\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (norm1): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm2): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (encoder4): UnetrBasicBlock(\n",
       "      (layer): UnetResBlock(\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (norm1): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm2): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (encoder10): UnetrBasicBlock(\n",
       "      (layer): UnetResBlock(\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(1536, 1536, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(1536, 1536, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (norm1): InstanceNorm3d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm2): InstanceNorm3d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (decoder5): UnetrUpBlock(\n",
       "      (transp_conv): Convolution(\n",
       "        (conv): ConvTranspose3d(1536, 768, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "      )\n",
       "      (conv_block): UnetResBlock(\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(1536, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (norm1): InstanceNorm3d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm2): InstanceNorm3d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (conv3): Convolution(\n",
       "          (conv): Conv3d(1536, 768, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (norm3): InstanceNorm3d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (decoder4): UnetrUpBlock(\n",
       "      (transp_conv): Convolution(\n",
       "        (conv): ConvTranspose3d(768, 384, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "      )\n",
       "      (conv_block): UnetResBlock(\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(768, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (norm1): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm2): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (conv3): Convolution(\n",
       "          (conv): Conv3d(768, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (norm3): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (decoder3): UnetrUpBlock(\n",
       "      (transp_conv): Convolution(\n",
       "        (conv): ConvTranspose3d(384, 192, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "      )\n",
       "      (conv_block): UnetResBlock(\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(384, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (norm1): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm2): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (conv3): Convolution(\n",
       "          (conv): Conv3d(384, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (norm3): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (decoder2): UnetrUpBlock(\n",
       "      (transp_conv): Convolution(\n",
       "        (conv): ConvTranspose3d(192, 96, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "      )\n",
       "      (conv_block): UnetResBlock(\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(192, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (norm1): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm2): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (conv3): Convolution(\n",
       "          (conv): Conv3d(192, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (norm3): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (decoder1): UnetrUpBlock(\n",
       "      (transp_conv): Convolution(\n",
       "        (conv): ConvTranspose3d(96, 96, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "      )\n",
       "      (conv_block): UnetResBlock(\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(192, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (norm1): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm2): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (conv3): Convolution(\n",
       "          (conv): Conv3d(192, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (norm3): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (out): UnetOutBlock(\n",
       "      (conv): Convolution(\n",
       "        (conv): Conv3d(96, 210, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model architecture (match the training setup)\n",
    "model = Swin3DSegmentation(input_channels=1, num_classes=210).to(device)\n",
    "\n",
    "# Load the state dictionary and map to CPU\n",
    "state_dict = torch.load(\"patch_model.pth\", map_location=\"cpu\")\n",
    "\n",
    "# Load the weights into the model\n",
    "model.load_state_dict(state_dict, strict = False)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "# Example Dice coefficient function\n",
    "def dice_coefficient(preds, targets, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Calculate the Dice Coefficient.\n",
    "    Args:\n",
    "        preds (torch.Tensor): Predicted binary or multi-class mask (logits or probabilities).\n",
    "        targets (torch.Tensor): Ground truth mask (same shape as preds).\n",
    "        smooth (float): Smoothing factor to avoid division by zero.\n",
    "    Returns:\n",
    "        float: Dice score.\n",
    "    \"\"\"\n",
    "    # Ensure predictions are binary (e.g., logits converted to binary masks)\n",
    "    preds = (preds > 0.5).float()\n",
    "    intersection = torch.sum(preds * targets)\n",
    "    union = torch.sum(preds) + torch.sum(targets)\n",
    "    dice = (2.0 * intersection + smooth) / (union + smooth)\n",
    "    return dice.item()\n",
    "\n",
    "# Load the dataset\n",
    "# Replace 'MRIDatasetWithPatches' and 'test_image_mask_pairs' with your dataset class and test data\n",
    "test_dataset = MRIDatasetWithPatches(\n",
    "    image_mask_pairs=val_pairs,  # Test dataset pairs\n",
    "    target_size=(64, 64, 64),                 # Replace with your patch size\n",
    "    stride=(32, 32, 32)                      # Replace with your stride\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "# Run evaluation\n",
    "dice_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "dice_scores = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculations for evaluation\n",
    "    for images, masks in test_loader:\n",
    "        images, masks = images.to(device), masks.to(device)  # Move to device\n",
    "\n",
    "        # Get predictions\n",
    "        outputs = model(images)  # Model output: logits or probabilities\n",
    "        # print(f\"Output shape: {outputs.shape}\")\n",
    "\n",
    "        probs = F.softmax(outputs, dim=1)  # Convert logits to probabilities (multi-class)\n",
    "        preds = torch.argmax(probs, dim=1)  # Get predicted class for each pixel\n",
    "        # print(f\"Predicted classes: {torch.unique(preds)}\")\n",
    "\n",
    "        # Calculate Dice coefficient for each valid class\n",
    "        valid_classes = torch.unique(masks)\n",
    "        # print(f\"Valid classes in ground truth: {valid_classes}\")\n",
    "        \n",
    "        for class_idx in valid_classes:\n",
    "            if class_idx == 0:  # Skip background\n",
    "                continue\n",
    "            dice = dice_coefficient((preds == class_idx).float(), (masks == class_idx).float())\n",
    "            dice_scores.append((class_idx.item(), dice))\n",
    "\n",
    "# Calculate average Dice score\n",
    "class_dice_scores = {class_idx: [] for class_idx, _ in dice_scores}\n",
    "for class_idx, dice in dice_scores:\n",
    "    class_dice_scores[class_idx].append(dice)\n",
    "\n",
    "average_dice_scores = {class_idx: np.mean(scores) for class_idx, scores in class_dice_scores.items()}\n",
    "\n",
    "# Print the results\n",
    "print(\"Dice Scores by Class:\")\n",
    "for class_idx, dice_score in average_dice_scores.items():\n",
    "    print(f\"Class {class_idx}: {dice_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
