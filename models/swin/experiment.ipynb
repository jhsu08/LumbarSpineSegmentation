{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iw1zSSJb3S4q",
    "outputId": "ab5d369a-ea21-4da6-e8a2-c5be93429740"
   },
   "outputs": [],
   "source": [
    "#!pip install torch torchvision timm SimpleITK nibabel numpy scipy matplotlib scikit-learn monai ‘monai[einops]’\n",
    "#!pip install git+https://github.com/DIAGNijmegen/Tiger.git@stable\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models.swin_transformer import SwinTransformer\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#brew install wget\n",
    "#brew install unzip\n",
    "# !wget https://zenodo.org/api/records/10159290/files-archive -O data.zip\n",
    "# !unzip data.zip\n",
    "# !unzip data/images.zip -d data/extracted_images\n",
    "# !unzip data/masks.zip -d data/extracted_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "If21IJnv33Vf"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "OsAicttL5xVz"
   },
   "outputs": [],
   "source": [
    "image_files = []\n",
    "\n",
    "# Find all the files in the extracted directory\n",
    "for root, _, files in os.walk('../../extracted_images/images'):\n",
    "    for file in files:\n",
    "        if file.endswith('.mha'):\n",
    "            image_files.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_files = []\n",
    "\n",
    "# Find all mask files in the extracted directory\n",
    "for root, _, files in os.walk('../../extracted_images/masks'):\n",
    "    for file in files:\n",
    "        if file.endswith('.mha'):  # Assuming masks have \"_mask\" in their filename\n",
    "            mask_files.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "image_mask_pairs = list(zip(image_files, mask_files))\n",
    "for image_file, mask_file in image_mask_pairs:\n",
    "    if os.path.basename(image_file) == os.path.basename(mask_file):\n",
    "        continue\n",
    "    else:\n",
    "        print(\"mismatched files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_padded_patches(volume, patch_size=(64, 64, 64), stride=(32, 32, 32)):\n",
    "    \"\"\"\n",
    "    Extract overlapping patches from a 3D volume with padding for patches that exceed the volume boundary.\n",
    "    Args:\n",
    "        volume (numpy array): Input 3D volume (depth, height, width).\n",
    "        patch_size (tuple): Size of the patch (depth, height, width).\n",
    "        stride (tuple): Stride for sliding window extraction.\n",
    "    Returns:\n",
    "        list: List of padded 3D patches.\n",
    "    \"\"\"\n",
    "    depth, height, width = volume.shape\n",
    "    patch_depth, patch_height, patch_width = patch_size\n",
    "    stride_d, stride_h, stride_w = stride\n",
    "\n",
    "    patches = []\n",
    "\n",
    "    for z in range(0, depth, stride_d):\n",
    "        for y in range(0, height, stride_h):\n",
    "            for x in range(0, width, stride_w):\n",
    "                # Extract patch and pad if necessary\n",
    "                patch = volume[z:z + patch_depth, y:y + patch_height, x:x + patch_width]\n",
    "                padded_patch = pad_volume(patch, patch_size)\n",
    "                patches.append(padded_patch)\n",
    "\n",
    "    return patches\n",
    "\n",
    "\n",
    "class MRIDatasetWithPatches(Dataset):\n",
    "    def __init__(self, image_mask_pairs, target_size=(64, 64, 64), stride=(32, 32, 32), transform=None):\n",
    "        \"\"\"\n",
    "        Dataset class for extracting padded patches.\n",
    "        Args:\n",
    "            image_mask_pairs (list): List of (image_path, mask_path) pairs.\n",
    "            target_size (tuple): Patch size for extraction.\n",
    "            stride (tuple): Stride for overlapping patches.\n",
    "            transform (callable, optional): Transformations to apply to the data.\n",
    "        \"\"\"\n",
    "        self.image_mask_pairs = image_mask_pairs\n",
    "        self.target_size = target_size\n",
    "        self.stride = stride\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_mask_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_file, mask_file = self.image_mask_pairs[idx]\n",
    "\n",
    "        # Load image and mask\n",
    "        image = sitk.ReadImage(image_file)\n",
    "        mask = sitk.ReadImage(mask_file)\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        image = sitk.GetArrayFromImage(image)\n",
    "        mask = sitk.GetArrayFromImage(mask)\n",
    "\n",
    "        # Extract padded patches\n",
    "        image_patches = extract_padded_patches(image, self.target_size, self.stride)\n",
    "        mask_patches = extract_padded_patches(mask, self.target_size, self.stride)\n",
    "\n",
    "        # Select a specific patch based on idx\n",
    "        patch_idx = idx % len(image_patches)  # Cycle through available patches\n",
    "        img_patch = image_patches[patch_idx]\n",
    "        mask_patch = mask_patches[patch_idx]\n",
    "\n",
    "        # Apply transformations (optional)\n",
    "        if self.transform:\n",
    "            img_patch, mask_patch = self.transform(img_patch, mask_patch)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        img_tensor = torch.tensor(img_patch, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "        mask_tensor = torch.tensor(mask_patch, dtype=torch.long)\n",
    "\n",
    "        return img_tensor, mask_tensor\n",
    "\n",
    "\n",
    "def pad_volume(volume, target_size):\n",
    "    \"\"\"\n",
    "    Pads a 3D volume to the target size.\n",
    "    Args:\n",
    "        volume (numpy array): Input 3D volume.\n",
    "        target_size (tuple): Target size (depth, height, width).\n",
    "    Returns:\n",
    "        numpy array: Padded volume.\n",
    "    \"\"\"\n",
    "    depth, height, width = volume.shape\n",
    "    target_depth, target_height, target_width = target_size\n",
    "\n",
    "    pad_depth = max(0, target_depth - depth)\n",
    "    pad_height = max(0, target_height - height)\n",
    "    pad_width = max(0, target_width - width)\n",
    "\n",
    "    padding = [\n",
    "        (0, pad_depth),  # Pad at the end only\n",
    "        (0, pad_height),\n",
    "        (0, pad_width),\n",
    "    ]\n",
    "\n",
    "    return np.pad(volume, padding, mode=\"constant\", constant_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, image_mask_pairs, target_size=(128, 128, 128), transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the MRIDataset.\n",
    "\n",
    "        Args:\n",
    "            image_mask_pairs (list of tuples): List of (image_path, mask_path) pairs.\n",
    "            target_size (tuple): Desired size for the resampled images and masks (depth, height, width).\n",
    "            transform (callable, optional): Transformations to apply to the data.\n",
    "        \"\"\"\n",
    "        self.image_mask_pairs = image_mask_pairs\n",
    "        self.target_size = target_size\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_mask_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_file, mask_file = self.image_mask_pairs[idx]\n",
    "\n",
    "        # Load image and mask\n",
    "        image = sitk.ReadImage(image_file)\n",
    "        mask = sitk.ReadImage(mask_file)\n",
    "\n",
    "        # Resample image and mask\n",
    "        image = self.resample_image(image, self.target_size, sitk.sitkLinear)\n",
    "        mask = self.resample_image(mask, self.target_size, sitk.sitkNearestNeighbor)\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        image = sitk.GetArrayFromImage(image)\n",
    "        mask = sitk.GetArrayFromImage(mask)\n",
    "\n",
    "        # Normalize the image\n",
    "        image = (image - np.min(image)) / (np.max(image) - np.min(image))\n",
    "        image = torch.tensor(image, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "        mask = torch.tensor(mask, dtype=torch.long)  # Long type for classification tasks\n",
    "\n",
    "        if self.transform:\n",
    "            image, mask = self.transform(image, mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    @staticmethod\n",
    "    def resample_image(image, target_size, interpolator):\n",
    "        \"\"\"\n",
    "        Resamples a 3D image to the target size.\n",
    "\n",
    "        Args:\n",
    "            image (SimpleITK.Image): The input image.\n",
    "            target_size (tuple): Desired size (depth, height, width).\n",
    "            interpolator (SimpleITK.Interpolator): Interpolation method (e.g., sitk.sitkLinear or sitk.sitkNearestNeighbor).\n",
    "\n",
    "        Returns:\n",
    "            SimpleITK.Image: Resampled image.\n",
    "        \"\"\"\n",
    "        original_size = image.GetSize()\n",
    "        original_spacing = image.GetSpacing()\n",
    "\n",
    "        # Calculate target spacing\n",
    "        target_spacing = [\n",
    "            (original_size[i] * original_spacing[i]) / target_size[i]\n",
    "            for i in range(3)\n",
    "        ]\n",
    "\n",
    "        # Resample the image\n",
    "        resample = sitk.ResampleImageFilter()\n",
    "        resample.SetOutputSpacing(target_spacing)\n",
    "        resample.SetSize(target_size)\n",
    "        resample.SetInterpolator(interpolator)\n",
    "        resample.SetOutputOrigin(image.GetOrigin())\n",
    "        resample.SetOutputDirection(image.GetDirection())\n",
    "        resample.SetDefaultPixelValue(0)\n",
    "\n",
    "        return resample.Execute(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MRIDatasetWithPatches(image_mask_pairs=image_mask_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split image-mask pairs into training and validation\n",
    "train_pairs, val_pairs = train_test_split(image_mask_pairs, test_size=0.2, random_state=55)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MRIDatasetWithPatches(image_mask_pairs=train_pairs)\n",
    "val_dataset = MRIDatasetWithPatches(image_mask_pairs=val_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0quJCaY4BCJ"
   },
   "source": [
    "# 3D Swin Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swin3DSegmentation(nn.Module):\n",
    "    def __init__(self, input_channels=1, num_classes=210, img_size=(64, 64, 64), embed_dim=96, patch_size=4):\n",
    "        \"\"\"\n",
    "        3D segmentation model using SwinUNETR backbone.\n",
    "        Args:\n",
    "            input_channels (int): Number of input channels (e.g., 1 for grayscale MRI images).\n",
    "            num_classes (int): Number of output classes for segmentation.\n",
    "            img_size (tuple): Size of the 3D input image (D, H, W).\n",
    "            embed_dim (int): Embedding dimension of the Swin Transformer.\n",
    "            patch_size (int): Patch size for splitting the input volume.\n",
    "        \"\"\"\n",
    "        super(Swin3DSegmentation, self).__init__()\n",
    "        \n",
    "        # SwinUNETR backbone\n",
    "        self.swin_unetr = SwinUNETR(\n",
    "            img_size=img_size,  # 3D image size\n",
    "            in_channels=input_channels,\n",
    "            out_channels=num_classes,\n",
    "            feature_size=embed_dim,\n",
    "            use_checkpoint=True  # Enables gradient checkpointing for memory efficiency\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for 3D segmentation.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, C, D, H, W).\n",
    "                              - B: Batch size\n",
    "                              - C: Number of input channels\n",
    "                              - D: Depth of the input image\n",
    "                              - H: Height of the input image\n",
    "                              - W: Width of the input image\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (B, num_classes, D, H, W).\n",
    "                          - B: Batch size\n",
    "                          - num_classes: Number of segmentation classes\n",
    "                          - D: Depth of the output image\n",
    "                          - H: Height of the output image\n",
    "                          - W: Width of the output image\n",
    "        \"\"\"\n",
    "        return self.swin_unetr(x)\n",
    "\n",
    "def save_model(model, path):\n",
    "    \"\"\"\n",
    "    Saves the model's state dictionary to the specified path.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model to be saved.\n",
    "        path: The file path where the model will be saved.\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs, device, save_path=\"trained_model.pth\"):\n",
    "    \"\"\"\n",
    "    Trains the model on the given data loaders and saves it after training.\n",
    "    Args:\n",
    "        model: The neural network model to be trained.\n",
    "        train_loader: DataLoader for the training data.\n",
    "        val_loader: DataLoader for the validation data.\n",
    "        epochs: Number of epochs to train.\n",
    "        device: Device for training (e.g., 'cpu', 'cuda', 'mps').\n",
    "        save_path: Path to save the trained model.\n",
    "    \"\"\"\n",
    "    # Define loss function and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        epoch_loss = 0.0\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Training loop with progress bar\n",
    "        for images, masks in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "            images, masks = images.to(device), masks.to(device)  # Move data to device\n",
    "\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs, masks)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():  # Disable gradient calculations for validation\n",
    "            for images, masks in val_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "    # Save the model after training\n",
    "    save_model(model, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCBxmDzn3i68"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "            self = reduction.pickle.load(from_parent)\n",
      "       ^^^^      ^^^^^^^^^^^^^^^^^^^^^^\n",
      "     ^^^^^^^^^^^^^  File \"/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'MRIDatasetWithPatches' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'MRIDatasetWithPatches' on <module '__main__' (built-in)>\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'MRIDatasetWithPatches' on <module '__main__' (built-in)>\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'MRIDatasetWithPatches' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'MRIDatasetWithPatches' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'MRIDatasetWithPatches' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'MRIDatasetWithPatches' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'MRIDatasetWithPatches' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 6514) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py:440\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 440\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py:948\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 948\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py:73\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[43m_error_if_any_worker_fails\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 6514) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Initialize the model and move it to the appropriate device\u001b[39;00m\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m Swin3DSegmentation(input_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m210\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 12\u001b[0m images, masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m train(model, train_loader, val_loader, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice, save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatch_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1414\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1256\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1255\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1257\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1258\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 6514) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "mp.set_start_method('spawn', force=True)  # Ensure proper start method for multiprocessing\n",
    "# Check device compatibility\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=8, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Initialize the model and move it to the appropriate device\n",
    "model = Swin3DSegmentation(input_channels=1, num_classes=210).to(device)\n",
    "images, masks = next(iter(train_loader))\n",
    "images = images.to(device)\n",
    "train(model, train_loader, val_loader, epochs=10, device=device, save_path=\"patch_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-9fKMG73qy9"
   },
   "source": [
    "# Evaluation"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
